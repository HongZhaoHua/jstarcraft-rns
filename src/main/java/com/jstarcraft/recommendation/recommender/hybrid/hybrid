集成学习综述:
http://rec-sys.net/plugin.php?id=freeaddon_pdf_preview:pdf&pid=697&aid=101&md5hash=2591efd25fffc39ab95630f1d5bc536b
集成学习分为异态集成学习和同态集成学习.
异态集成学习又分为叠加法(Stack Generalization)和元学习法(Meta Learning).
叠加法(Stack Generalization)实际就是Stacking
元学习法主要有仲裁法(arbiter)和合并法(combiner)
Bagging和Boosting属于合并法.

使用集成算法的时候,可以按照一个基本准则:
如果场景中的model为同质且大多数model都是欠拟合(偏差大,方差小),则选择boosting.
如果场景中的model为异质且大多数model都是欠拟合(偏差大,方差小),则选择stacking
如果场景中的model为同质且大多数model都是过拟合(偏差小,方差大),则选择bagging.
如果场景中的model为异质且大多数model都是效果精准(偏差不高,方差也不高),则选择stacking.

笔记︱集成学习Ensemble Learning与树模型、Bagging 和 Boosting、模型融合:
http://blog.csdn.net/sinat_26917383/article/details/54667077

机器学习-->集成学习-->Bagging,Boosting,Stacking
http://blog.csdn.net/Mr_tyting/article/details/72957853

集成学习 - bagging/boosting/stacking:
https://www.zhihu.com/question/29036379

【译文】集成学习三大法宝-bagging、boosting、stacking
https://zhuanlan.zhihu.com/p/36161812

混合推荐技术总结:
https://my.oschina.net/liangtee/blog/119106

深入浅出混合推荐技术:
https://arthur503.github.io/blog/2013/09/30/mixed-recommend-systems.html

魅族推荐平台架构解析:
http://blog.csdn.net/tech_meizu/article/details/70207570

推荐系统中所使用的混合技术介绍:
https://www.52ml.net/318.html

在业界实际部署时,解决此类常见问题的流行方法是采用三段式混合系统：
即Online-Nearline-Offline Recommendation(在线-近线-离线)三层混合机制.
其中Online系统直接面向用户,是一个高性能和高可用性的推荐服务,在这里通常会设计有缓存(Cache)系统,来处理热门的请求(Query)重复计算的问题.
而当Cache不命中的情况下,Online推荐运行一个运算简单可靠的算法,及时生成结果.
Online系统后是Nearline系统,这个系统部署在服务端,一方面会接收Online系统发过来的请求,将Online计算的一些缓存结果,采用更复杂的算法重新计算并更新后更新缓存.
另一方面Nearline是衔接Online和Offline系统的桥梁,因为Offline结果往往会挖掘长期的、海量的用户行为日志,消耗的资源大、挖掘周期长,但是Offline推荐系统计算所得的结果质量往往是最高的,这些结果会通过Nearline系统输送到线上,发挥作用.

混合推荐系统是推荐系统的另一个研究热点,它是指将多种推荐技术进行混合相互弥补缺点,从而可以获得更好的推荐效果.
最常见的是将协同过滤技术和其他技术相结合,克服cold start的问题.

 整体式策略:

(1)特征组合型 - Feature Combination
将来自不同推荐数据源的特征组合起来,由另一种推荐技术采用.
一般会将协同过滤的信息作为增加的特征向量,然后在这增加的数据集上采用基于内容的推荐技术.
Feature Combination的混合方式使得系统不再仅仅考虑协同过滤的数据源,所以它降低了用户对项目评分数量的敏感度,相反的,它允许系统拥有项的内部相似信息,其对协同系统是不透明的.

理解:
特征组合其实就是将不同数据源的数据通过预处理形成更丰富的特征集合,再将它们作为输入数据的一部分一起输入到推荐算法中.

疑问:
1,按照此理解,特征组合型的策略,核心推荐算法一定类似content-based或者factorization machine之类能够处理特征的算法.对应到LibRec中就是TensorRecommender.
2,现在流行的基于标签的推荐,也是特征组合型的思想的泛化.

(2)特征递增/特征扩充型 - Feature Augmentation
前一个推荐方法的输出作为后一个推荐方法的输入.
比如,你可以将聚类分析作为关联规则的预处理,首先对会话文件进行聚类,再针对每个聚类进行关联规则挖掘,得到不同聚类的关联规则.当一个访问会话获得后,首先计算该访问会话与各聚类的匹配值,确认其属于哪个聚类,再应用这个聚类对应的关联规则进行推荐.
这个类型和瀑布型的不同点在哪里呢？
在特征递增型中,第二种推荐方法使用的特征包括了第一种的输出.
而在瀑布型中,第二种推荐方法并没有使用第一种产生的任何等级排列的输出,其两种推荐方法的结果以一种优化的方式进行混合.

理解:
Feature Augmentation其实就是在预处理阶段采用机器学习的其它方法,补全缺失的数据或者挖掘新的特征,再将它们作为输入数据的一部分一起输入到推荐算法中.

疑问:
1,按照此理解,特征递增型的策略,核心推荐算法可以是只支持分数矩阵输入的itemknn或者userknn,也可以是能支持特征矩阵输入的content-based或者factorization machine.

总结:
整体式的两种策略似乎强调的都是数据预处理(包括特征工程等),算法会根据具体的场景选择直接使用旧的算法,或者设计新的算法(通常是对旧方法执行少量的修改).
整体式的数据预处理似乎与现代的特征工程息息相关.
整体式的算法思想强调的都是改进旧的算法或者设计新的算法.(例如lambdaFM就是借鉴了BRP与FM思想,又例如SBPR借鉴了BPR与社交特征.)

注意:
Feature Augmentation与Cascade的区别
1,Feature Augmentation强调的是预处理阶段对数据的补全或者特征的挖掘.核心算法数量永远是1.
2,Cascade强调的是直接将上个算法的输出作为下个算法的输入.核心算法数量是>=2.

 并行式策略:

(3)合并/复合/交叉型 - Mix
同时采用多种推荐技术给出多种推荐结果,为用户提供参考.
比如,可以构建这样一个基于日志和缓存数据挖掘的个性化推荐系统,该系统首先通过挖掘日志和缓存数据构建用户多方面的兴趣模式,然后根据目标用户的短期访问历史与用户的长期兴趣模式进行匹配,采用基于内容的过滤算法,向用户推荐相似网页.
同时,通过对多用户间的协同过滤,为目标用户预测下一步最有可能的访问页面,并根据得分对页面进行排序,附在现行用户请求访问页面后推荐给用户.也就是“猜你喜欢可能感兴趣的网页”.

理解:
Mix实际是一种相当简单的策略,就是将不同推荐算法/系统的推荐结果聚合在一起.各个算法与系统之间既无关联也互不影响.

疑惑:
此种策略似乎有时候需要额外的筛选机制保证各个推荐算法的结果是相关的?
 
(4)加权型 - Weight
就是将多种推荐技术的计算结果加权混合产生推荐.
最简单的方式是线性混合,首先将协同过滤的推荐结果和基于内容的推荐结果赋予相同的权重值,然后比较用户对项的评价与系统的预测是否相符,然后调整权重值.
加权型混合方式的特点是整个系统性能都直接与推荐过程相关,这样一来就很容易在这之后进行信任分配和调整相应的混合模型,不过这种技术有一个假设的前提是对于整个空间中所有可能的项,使用不同技术的相关参数值都基本相同.

理解:
Weight实际是Mix的改进,就是将不同推荐算法/系统的推荐结果根据不同的权重(静态或者动态)调整以后得到最终的评分或者排序.

疑惑:
此种策略似乎有时候需要额外的权重机制,根据推荐效果调整各个推荐算法/系统的权重值?

(5)转换型 - Switch
根据问题背景和实际情况采用不同的推荐技术.
比如,使用基于内容推荐和协同过滤混合的方式,系统首先使用基于内容的推荐技术,如果它不能产生高可信度的推荐,然后再尝试使用协同过滤技术.
因为需要各种情况比较转换标准,所以这种方法会增加算法的复杂度和参数化,当然这样做的好处是对各种推荐技术的优点和弱点比较敏感.

理解:
Switch实际是Weight的改进,根据系统状态(冷启动等)或者用户所处的特定情景(上下文)选择不同推荐算法/系统为其服务.

疑惑:
此种策略似乎需要额外的裁定机制或者知识系统,决定在什么情况下使用哪种推荐算法/系统?

总结:
并行式的三种策略似乎都需要额外的协调机制配合才能有效的工作.

 串行式策略:

(6)瀑布/层叠型 - Cascade
后一个推荐方法优化前一个推荐方法：它是一个分阶段的过程,首先用一种推荐技术产生一个较为粗略的候选结果,在此基础上使用第二种推荐技术对其作出进一步精确地推荐.
瀑布型允许系统对某些项避免采用低优先级的技术,这些项可能是通过第一种推荐技术被较好的予以区分了的,或者是很少被用户评价从来都不会被推荐的项目.
因为瀑布型的第二步,仅仅是集中在需要另外判断的项上.另外,瀑布型在低优先级技术上具有较高的容错性,因为高优先级得出的评分会变得更加精确,而不是被完全修改.

理解:
Cascade实际是将上一个算法/系统的推荐列表作为下一个算法/系统的输入.(分数矩阵与特征矩阵依然可以和推荐列表一起作为下一个算法的输入)
上一个算法的推荐列表会限定下一个算法的推荐范围.
例如:基于知识的推荐列表通常是无序的或者得分相同的项大量存在,可以使用其它算法配合进一步优化推荐列表.

注意:
下一个算法永远只能改变上个算法的推荐列表的得分或者数量.

(7)元层次/分级型 - Meta Level
用一种推荐方法产生的模型作为另一种推荐方法的输入.
这个与特征递增型的不同在于：
在特征递增型中使用一个学习模型产生某些特征作为第二种算法的输入,而在元层次型中,整个模型都会作为输入.
比如,你可以通过组合基于用户的协同过滤和基于项目的协同过滤算法,先求解目标项目的相似项目集,在目标项目的相似项目集上再采用基于用户的协同过滤算法.这种基于相似项目的邻居用户协同推荐方法,能很好地处理用户多兴趣下的个性化推荐问题,尤其是候选推荐项目的内容属性相差很大的时候,该方法性能会更好.

理解:
Meta Level实际上是将上一个机器学习算法的模型结果值作为下一个机器学习算法的模型的初始值.

疑问:
1,按照此理解,各个算法/系统之间的模型至少在一定程度是相似的.例如:matrix factorization的各个算法都会有user factors与item factors,才能够实现Meta Level.
2,按照此理解,Meta Level能够组合的算法一定会限定在机器学习范畴.

总结:
串行式的两种策略似乎都强调算法层面的组合,只要满足条件,各个算法之间可以随意增删或者改变顺序,不会影响到其它系统.